{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AM207 Project: Mastermind\n",
    "\n",
    "### Expectation Maximization\n",
    "For this project we also tried to deploy expectation maximization (EM) to solve for the hidden code. At first sight this may seem very intuitive, since the parameters of the problem—the colors, or numbers, in the hidden code—are unobserved. Treating this like a latent parameter, like on does in expectation-maximization may seem like a good approach to take.\n",
    "\n",
    "Trying to work out the mathematics of a Mastermind expectation maximization strategy yields the following. EM is typically used in problems where the likelihood is hard to evaluate directly. Therefore, we recursively maximize the expectation of the likelihood conditional on the data, until we converge to a set of parameters that is close to the maximum likelihood estimator. In Mastermind, however, the likelihood of the data given a set of parameters is not so hard to evaluate at all. Since the constraint on the problem is that the hidden code should logically allow for the number of black and white pegs appearing given a certain guess, there can be many hidden codes with a likelihood of 1, while the rest will have likelihood 0. For example, if my guess is 1234, and as a response I get back two black pegs, then I know two numbers are right. Any hidden code where two numbers are in the same position and the rest are not, could validly produce this outcome, e.g. the code 1255 or 1664, etcetera. Given that such a code is true, the likelihood that the response was two black pegs is 1, since this is deterministically produced by the rules of the game. Therefore, there will be a split in the solution space between codes that are possible given the responses (likelihood equals 1), and codes that are impossible (likelihood equals 0). EM, in this case, would then converge to some solution in the set of still allowable codes. Since the choice of a new code isn't directed by any possible information gains, this amounts to a random search across the space. If we make an informed decision here, the problem boils down to a strategy where one would maximize the information gain, for instance using entropy or some other metric.\n",
    "\n",
    "In conclusion, EM is probably not a good way to study the Mastermind problem, since the game is underconstrained and deterministic, leading to a space of many equally likely convergent solutions at any given round in the game. It may be more useful in estimating parameters of stochastic processes with a lot of data, rather than in making forward looking decisions where information gain plays a bigger role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MMboard import MMboard \n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Expectation Maximization, but more like maximum likelihood actually\n",
    "\n",
    "def expectation_maximization(n_code, n_colors, code=None, silent=True):\n",
    "    \n",
    "    # Get board ready to play\n",
    "    board = MMboard(codelength=n_code, numcolors=n_colors, suppress_output=silent)\n",
    "    board.set_code() if not code else board.set_code(code)\n",
    "    \n",
    "    # Value stores\n",
    "    solutions = np.array(list(itertools.product(range(n_colors), repeat=n_code)))\n",
    "    guesses = []\n",
    "    responses = []\n",
    "    \n",
    "    # Emulates response given some hidden code\n",
    "    def response(guess, code):\n",
    "        equals = guess == code\n",
    "        in_both = np.intersect1d(guess[equals == False], code[equals == False])\n",
    "        guess_count = np.array(map(lambda a: np.count_nonzero(code[equals == False] == a), in_both))\n",
    "        code_count = np.array(map(lambda a: np.count_nonzero(guess[equals == False] == a), in_both))\n",
    "        wrong_places = np.array(map(lambda (a, b): a if a <= b else b, zip(guess_count, code_count)))\n",
    "        return len(equals.nonzero()[0]), int(wrong_places.sum())\n",
    "    \n",
    "    # Likelihood defined as above\n",
    "    likelihood = lambda guess, outcome, code: 1 if response(guess, code) == outcome else 0\n",
    "    \n",
    "    n_guesses = 0\n",
    "    \n",
    "    # Run as long we're not game over\n",
    "    while not board.gameover:\n",
    "        \n",
    "        # Make an informed guess from set of solutions with maximum likelihood\n",
    "        guess = solutions[np.random.randint(0, len(solutions))]\n",
    "        outcome = board.guess_code(guess)\n",
    "        print outcome, response(guess, board._code)\n",
    "        guesses.append(guess)\n",
    "        responses.append(outcome)\n",
    "        \n",
    "        # Compute feasible solutions left\n",
    "        valid_indices = []\n",
    "        for i in xrange(len(solutions)):\n",
    "            solution = solutions[i]\n",
    "            total_likelihood = 1\n",
    "            \n",
    "            # Compute likelihood over past outcomes\n",
    "            for j in xrange(len(guesses)):\n",
    "                guess = guesses[j]\n",
    "                outcome = responses[j]\n",
    "                if likelihood(guess, outcome, solution) == 0:\n",
    "                    total_likelihood = 0\n",
    "                    break\n",
    "            \n",
    "            # Keep only solutions with likelihood > 0\n",
    "            if total_likelihood > 0:\n",
    "                valid_indices.append(i)\n",
    "        \n",
    "        # Set solutions\n",
    "        solutions = solutions[valid_indices]\n",
    "        \n",
    "        # Set guess count\n",
    "        n_guesses += 1\n",
    "        \n",
    "    return n_guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code successfully initialized to  [3 5 0 3] \n",
      "\n",
      "guess #1 of 10: you guessed  [1 3 2 3]\n",
      "You have 1 right item(s) in the right place, and\n",
      "  1 right item(s) but in the wrong place\n",
      "\n",
      "(1, 1) (1, 1)\n",
      "guess #2 of 10: you guessed  [3 5 2 0]\n",
      "You have 2 right item(s) in the right place, and\n",
      "  1 right item(s) but in the wrong place\n",
      "\n",
      "(2, 1) (2, 1)\n",
      "guess #3 of 10: you guessed  [5 1 2 0]\n",
      "You have 0 right item(s) in the right place, and\n",
      "  2 right item(s) but in the wrong place\n",
      "\n",
      "(0, 2) (0, 2)\n",
      "guess #4 of 10: you guessed  [3 5 0 3]\n",
      "You have 4 right item(s) in the right place\n",
      "You win!\n",
      "(4, 0) (4, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_maximization(4, 6, silent=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
