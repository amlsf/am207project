\documentclass[11pt]{article}
\usepackage{common}
\usepackage[usenames, dvipsnames]{color}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{multirow}

\title{AM207 Final Project: Mastermind}
\author{Gioia Domined\`o \and Amy Lee \and Kendrick Lo \and Reiner Maat}

\begin{document}
\maketitle{}

\begin{abstract}
\textcolor{red}{Insert abstract text.}
\end{abstract}

\pagestyle{plain}
\pagenumbering{arabic}

\section{Introduction}

The game Mastermind was invented in 1970 by Mordecai Meirowitz, and is similar to an earlier game called Bulls and Cows. There are many variations of the game\footnote{https://en.wikipedia.org/wiki/Mastermind\_(board\_game)\#Variations}, but they all follow a broadly consistent format. The game involves two players: a code-maker and a code-breaker. The code-maker selects a sequence of length L (usually 4), where each element in the sequence is chosen with replacement from one of C colors (usually 6). At each turn, the code-breaker guesses a sequence and receives feedback in the form of black pegs and white pegs, where the black pegs denote the number of correct guesses in the right position, and the white pegs denote the number of correct guesses in the wrong position. Based on this information, the code-breaker refines her guesses and attempts to crack the code within the maximum number of guesses (usually 10).

For our project, we set out to implement various strategies and algorithms for iteratively optimizing the code-breaker's guess at each turn. We compared the performance of these strategies based on the mean and standard deviation of the required number of guesses to win and the runtime across 20 random game initializations. In order to assess the extent to which the different solutions scale efficiently, we tested seven game setups of varying complexity.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.7\textwidth]{img/game_setup}
\caption{Mastermind Framework}
\label{fig:game_setup}
\end{figure}

\newpage

\section{Optimization Methods}

It is helpful at this stage to introduce some mathematical notation to describe the game, which we will use consistently when describing the various methods that we tested. For simplicity, we use one-indexing in the formulas below; however, we note that the Python code for the game interface that we built is zero-indexed. \medskip

\noindent We define:
\begin{itemize}
\item set of possible colors: $C = \{1, ..., C\}$
\item set of positions in the code: $L = \{1, ..., L\}$
\item hidden code: $H_i \ \forall i \in L$
\item guess of the hidden code: $T_i \ \forall i \in L$
\item indicator function $\mathbb{1}_{A=B}$, which equals 1 if A=B and equals 0 otherwise
\end{itemize}

\noindent Using the above notation, we can denote the responses at each turn as follows:

\begin{itemize}
\item correct guesses in the right position: $B = \sum_{i=1}^L \mathbb{1}_{T_i=H_i} \ \forall i \in L$
\item correct guesses in the wrong position: $W = \sum_{i=1}^{C} \min(\sum_{j=1}^{L}\mathbb{1}_{H_j=i, G_i}, \sum_{j=1}^{L}\mathbb{1}_{T_j=i, G_i}) - B$
\end{itemize}

\subsection{Knuth's Five-Guess Algorithm}

The most commonly referenced optimization technique in the Mastermind literature is Knuth's five-guess algorithm \cite{knuth76} \textendash \ sometimes also referred to as the ``worst-case strategy" \textendash \ which can always solve the classic game configuration in five moves or less. This strategy always begins with the same initial guess of 1122 (or 0011, when zero-indexed); this choice is motivated by examples of other initial guesses that do not always lead to a solution in five moves. Our implementation uses this deterministic initial guess for the classic game configuration, but uses a random initial guess for all other configurations as analogous ``best" starting points are not defined in the literature.

After the initial guess, the algorithm determines the minimum number of codes that each guess would eliminate from the list of possibilities, and chooses one of the guesses that maximizes this number\footnote{Wikipedia refers to this as the minimax \textit{technique}. We note that this one-off maximization differs from the recursive minimax \textit{algorithm} that is also used in game theory.}. At each stage of the game, the set of possible codes is updated to only include codes that would have generated the same responses at each of the previous steps. This ensures that the state space shrinks with each subsequent guess.

\subsection{Random Search with Constraints / Random Sampling From Posterior}

This method is described in the Mastermind literature both as a constrained random search algorithm \cite{bernier1996solving} and in terms of posterior distribution updates \cite{vomlel2004bayesian}. We follow the latter approach below.

We start by defining the joint probability distribution over all possible code sequences as $P(H_1, ..., H_L)$. As we have no information, our prior is uniformly distributed.

\[
P(H_1=h_1, ..., H_L=h_l) = \frac{1}{C^L} ,\quad \text{for all combinations of }(h_1, ..., h_l)
\]

\noindent We can denote the evidence that obtain at each step as $e = (B, W)$, where B and W are defined as above, and use this to update the posterior joint distribution over code sequences as follows:

\[
    P(H_1=h_1, ..., H_L=h_l | e) = 
\begin{cases}
    \frac{1}{|s(e)|},& \text{if } (h_1, ..., h_l) \ \text{is a possible code}\\
    0,              & \text{otherwise}
\end{cases}
\]

\noindent where s(e) denotes the set of possible hidden codes, given the evidence, and $|s(e)|$ denotes the cardinality of this set. \medskip

\noindent We can define the posterior after multiple game steps analogously:

\[
    P(H_1=h_1, ..., H_L=h_l | e_1, ..., e_n) = 
\begin{cases}
    \frac{1}{| s(e_1) \ \cap \ ... \ \cap \ s(e_n) |},& \text{if } (h_1, ..., h_l) \ \text{is a possible code}\\
    0,              & \text{otherwise}
\end{cases}
\]

\noindent where $\ s(e_1) \ \cap \ ... \ \cap \ s(e_n)$ denotes the intersection of the sets of possible hidden codes, given the evidence at each step, and the entire denominator denotes the cardinality of this intersection.

We can use this framework to define the posterior updates at each round of the game, and then choose the next guess by sampling from the posterior distribution. Figure \ref{fig:num_guesses} shows how the number of possible guesses shrinks as the game progresses.

\subsection{Maximizing Shannon Entropy}

Entropy is a measure that is commonly used in information theory to quantify the average amount of information that is contained in a message. In the context of Mastermind, the ``message" is the response of black and white pins that is returned by the code-maker at each turn. The goal of the code-breaker is to choose guesses that create as even a distribution as possible between the various responses\footnote{http://www.geometer.org/mathcircles/mastermind.pdf}, as it will allow her to discard more possible codes at the next step.

Let us denote $r_i$ as the $i$th response category and R as the number of possible responses\footnote{For example, the classic version of the game with codes of length $4$ has 14 possible responses: $(4, 0), (3, 0), (2, 2), (2, 1), (2, 0), (1, 3), (1, 2), (1, 1), (1, 0), (0, 4), (0, 3), (0, 2), (0, 1), (0, 0)$.}. We can then define the entropy of the discrete response space $\{r_1, ... , r_R\}$ for a given guess as:

\[
H(\text{guess} | \text{possible codes}) = \sum_{i=1}^R P(r_i) I(r_i) = - \sum_{i=1}^R P(r_i) \log_bP(r_i)
\]

\noindent where $I(r_i)$ denotes the information content of the $i$th response category. We use $b=2$, meaning that we are measuring entropy in shannons, but note that any other value of $b$ would yield a consistent ranking between guesses.

Practically, we calculate the probability of each response category for a given guess by counting (and normalizing) the total number of possible responses in each category, given the hidden codes that are still possible at that particular stage in the game. The value will depend on the shape of the probability distribution across the response categories, with the minimum entropy of $0$ only achievable when there is certainty of a particular outcome (i.e. $log_2(1)=0$).

In order to improve her performance, the code-breaker will want to pick the guess that results in the highest entropy \textendash \ or, if there are ties, one of the best guesses \textendash \ in order to be able to discard more possible codes at the next step. This can be achieved through an exhaustive calculation of the entropy of all possible guesses at each stage or, for larger state spaces, through a local search technique such as simulated annealing or genetic algorithms.  Figure \ref{fig:entropy} illustrates how the distribution of entropy for the remaining possible guesses can change as the game progresses; this technique is particularly effective where there is a clear maximum value (e.g. at the third guess).

\subsection{Simulated Annealing}

Knuth's algorithm and maximizing shannon entropy are both examples of global optimization techniques. These approaches work well when optimizing across relatively small state spaces, but they are not guaranteed to scale well as the problem size increases. For example, maximizing shannon entropy has complexity $O(|s(e_i^2)|)$ at the $i$th round of the game, i.e. the runtime scales quadratically with the number of possible codes.

Simulated annealing provides an alternative optimization approach that does not require an exhaustive search of the state space\footnote{For the sake of brevity, the simulated annealing algorithm is not described here.}. The effectiveness of this technique will depend upon the objective function that is used to evaluate potential guesses, and the method that is used to propose potential new guesses. As mentioned above, we can use simulated annealing as a faster alternative to calculating the shannon entropy of all guesses. In this case, new guesses will be proposed by randomly picking from the set of remaining possible guesses, and will be evaluated by calculating their entropy.

An alternative approach is described by Bernier et al. \cite{bernier1996solving}. Under this framework, new guesses are proposed by randomly picking from the set of all guesses that have not already been used. Not updating the set of possible guesses at each turn reduces the runtime complexity of the algorithm. The proposed guess's objective function is defined as the difference between the black and white pegs obtained from previous guesses and the number of black and white pegs that would have been obtained if the newly proposed guess was the right code. Once again, this approach has the advantage of being computationally inexpensive.

\subsection{Genetic Algorithms}

\noindent \textcolor{red}{TODO}

\section{Experiments}

\noindent \textcolor{red}{TODO}

%variations on entropy - exclude first guess

\section{Conclusion}

\noindent \textcolor{red}{TODO}

%Gagneur & co - applications to selective phenotyping

\newpage
\appendix

\section{Figures}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.6\textwidth]{img/num_guesses}
\caption{Evolution in number of possible guesses}
\label{fig:num_guesses}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.6\textwidth]{img/entropy}
\caption{Entropy distributions for classic game parameters}
\label{fig:entropy}
\end{figure}

\newpage
\section{Results}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{\bfseries Optimization Method} 		& \multicolumn{2}{c}{\bfseries Number of Guesses} 		& \multicolumn{2}{c}{\bfseries Runtime (Seconds)}	\\
\cmidrule(lr){2-3}  \cmidrule(lr){4-5}				& $\mu$ & $\sigma$								& $\mu$ & $\sigma$							\\
\cmidrule(lr){1-5}
Knuth's Algorithm							& 4.0 & 0.000									& 0.021 & 0.002							\\
Random Search/Random Sampling				& 3.7 & 0.557									& 0.002 & 0.000							\\
Maximizing Entropy (All Steps)					& 3.8 & 0.510									& 0.193 & 0.010							\\
Maximizing Entropy (Except First Step)			& 3.7 & 0.640									& 0.007 & 0.004							\\
Simulated Annealing (Bernier Objective Function)	& 7.5 & 2.037									& 0.017 & 0.026							\\
Simulated Annealing (Entropy Objective Function)	& 3.5 & 0.805									& 0.003 & 0.001							\\
Genetic Algorithms (XX Objective Function)		& &											& &										\\
\bottomrule
\end{tabular}
\end{center}
\caption{4 Positions; 4 Possible Digits}
\label{fig:compare_4_4}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{\bfseries Optimization Method} 		& \multicolumn{2}{c}{\bfseries Number of Guesses} 		& \multicolumn{2}{c}{\bfseries Runtime (Seconds)}	\\
\cmidrule(lr){2-3}  \cmidrule(lr){4-5}				& $\mu$ & $\sigma$								& $\mu$ & $\sigma$							\\
\cmidrule(lr){1-5}
Knuth's Algorithm							& 4.0 & 0.000									& 0.264 & 0.006							\\
Random Search/Random Sampling				& 4.5 & 0.742									& 0.006 & 0.001							\\
Maximizing Entropy (All Steps)					& 4.0 & 0.497									& 1.147 & 0.035							\\
Maximizing Entropy (Except First Step)			& 4.0 & 0.894									& 0.042 &	0.017							\\
Simulated Annealing (Bernier Objective Function)	& 8.0 & 1.359									& 0.042 & 0.129							\\
Simulated Annealing (Entropy Objective Function)	& 4.7 & 0.781									& 0.007 & 0.001							\\
Genetic Algorithms (XX Objective Function)		& &											& &										\\
\bottomrule
\end{tabular}
\end{center}
\caption{4 Positions; 5 Possible Digits}
\label{fig:compare_4_5}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{\bfseries Optimization Method} 		& \multicolumn{2}{c}{\bfseries Number of Guesses} 		& \multicolumn{2}{c}{\bfseries Runtime (Seconds)}	\\
\cmidrule(lr){2-3}  \cmidrule(lr){4-5}				& $\mu$ & $\sigma$								& $\mu$ & $\sigma$							\\
\cmidrule(lr){1-5}
Knuth's Algorithm							& 4.0 & 0.000									& 0.606 & 0.017							\\
Random Search/Random Sampling				& 4.8 & 0.994									& 0.013 & 0.003							\\
Maximizing Entropy (All Steps)					& 4.8 & 0.600									& 5.225 & 0.207							\\
Maximizing Entropy (Except First Step)			& 4.4 & 0.663									& 0.145 & 0.099							\\
Simulated Annealing (Bernier Objective Function)	& 8.0 & 1.844									& 0.268 & 0.421							\\
Simulated Annealing (Entropy Objective Function)	& 5.0 & 0.949									& 0.013 & 0.001							\\
Genetic Algorithms (XX Objective Function)		& &											& &										\\
\bottomrule
\end{tabular}
\end{center}
\caption{4 Positions; 6 Possible Digits}
\label{fig:compare_4_6}
\end{table}

\newpage

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{\bfseries Optimization Method} 		& \multicolumn{2}{c}{\bfseries Number of Guesses} 		& \multicolumn{2}{c}{\bfseries Runtime (Seconds)}	\\
\cmidrule(lr){2-3}  \cmidrule(lr){4-5}				& $\mu$ & $\sigma$								& $\mu$ & $\sigma$							\\
\cmidrule(lr){1-5}
Knuth's Algorithm							& 4.0 & 0.000									& 0.528 & 0.014							\\
Random Search/Random Sampling				& 5.0 & 1.023									& 0.023 & 0.002							\\
Maximizing Entropy (All Steps)					& 4.8 & 0.812									& 18.600 & 0.883							\\
Maximizing Entropy (Except First Step)			& 4.8 & 0.766									& 0.669 & 0.407							\\
Simulated Annealing (Bernier Objective Function)	& 9.2 & 1.077									& 0.069 & 0.224							\\
Simulated Annealing (Entropy Objective Function)	& 5.3 & 0.781									& 0.028 & 0.005							\\
Genetic Algorithms (XX Objective Function)		& &											& &										\\
\bottomrule
\end{tabular}
\end{center}
\caption{4 Positions; 7 Possible Digits}
\label{fig:compare_4_7}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{\bfseries Optimization Method} 		& \multicolumn{2}{c}{\bfseries Number of Guesses} 		& \multicolumn{2}{c}{\bfseries Runtime (Seconds)}	\\
\cmidrule(lr){2-3}  \cmidrule(lr){4-5}				& $\mu$ & $\sigma$								& $\mu$ & $\sigma$							\\
\cmidrule(lr){1-5}
Knuth's Algorithm							& 6.0 & 0.000									& 4.663 & 0.269							\\
Random Search/Random Sampling				& 5.4 & 1.114									& 0.044 & 0.005							\\
Maximizing Entropy (All Steps)					& 5.1 & 0.943									& 53.689 & 1.9444							\\
Maximizing Entropy (Except First Step)			& 5.4 & 0.735									& 1.464 & 1.162							\\
Simulated Annealing (Bernier Objective Function)	& 9.1 & 1.578									& 0.290 & 0.478							\\
Simulated Annealing (Entropy Objective Function)	& 5.5 & 1.023									& 0.043 & 0.005							\\
Genetic Algorithms (XX Objective Function)		& &											& &										\\
\bottomrule
\end{tabular}
\end{center}
\caption{4 Positions; 8 Possible Digits}
\label{fig:compare_4_8}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{\bfseries Optimization Method} 		& \multicolumn{2}{c}{\bfseries Number of Guesses} 		& \multicolumn{2}{c}{\bfseries Runtime (Seconds)}	\\
\cmidrule(lr){2-3}  \cmidrule(lr){4-5}				& $\mu$ & $\sigma$								& $\mu$ & $\sigma$							\\
\cmidrule(lr){1-5}
Knuth's Algorithm							& 7.0 & 0.000									& 67.299 & 1.928							\\
Random Search/Random Sampling				& 5.9 & 0.995									& 0.069 & 0.008							\\
Maximizing Entropy (All Steps)					& 5.8 & 1.043									& 149.057 & 11.014							\\
Maximizing Entropy (Except First Step)			& 5.7 & 0.714									& 5.702 & 4.282							\\
Simulated Annealing (Bernier Objective Function)	& 8.8 & 1.824									& 0.191 & 0.331							\\
Simulated Annealing (Entropy Objective Function)	& 6.2 & 1.288									& 0.091 & 0.021							\\
Genetic Algorithms (XX Objective Function)		& &											& &										\\
\bottomrule
\end{tabular}
\end{center}
\caption{4 Positions; 9 Possible Digits}
\label{fig:compare_4_9}
\end{table}

\newpage

\begin{table}[h!]
\begin{center}
\begin{tabular}{l c c c c}
\toprule
\multirow{2}{*}{\bfseries Optimization Method} 		& \multicolumn{2}{c}{\bfseries Number of Guesses} 		& \multicolumn{2}{c}{\bfseries Runtime (Seconds)}	\\
\cmidrule(lr){2-3}  \cmidrule(lr){4-5}				& $\mu$ & $\sigma$								& $\mu$ & $\sigma$							\\
\cmidrule(lr){1-5}
Knuth's Algorithm							& 6.0 & 0.000									& 84.160 & 9.606							\\
Random Search/Random Sampling				& 6.6 & 1.020									& 0.098 & 0.006							\\
Maximizing Entropy (All Steps)					& 5.8 & 1.062									& 344.603 & 21.921							\\
Maximizing Entropy (Except First Step)			& 6.0 & 0.775									& 13.557 & 9.339							\\
Simulated Annealing (Bernier Objective Function)	& 9.4 & 0.970									& 0.248 & 0.513							\\
Simulated Annealing (Entropy Objective Function)	& 6.0 & 1.304									& 0.127 & 0.009							\\
Genetic Algorithms (XX Objective Function)		& &											& &										\\
\bottomrule
\end{tabular}
\end{center}
\caption{4 Positions; 10 Possible Digits}
\label{fig:compare_4_10}
\end{table}

\newpage

%necessary for non-cited references to show up
\nocite{runarsson2010adapting}
\nocite{merelo2010finding}
\nocite{doerr2013playing}
\nocite{merelo2013improving}
\nocite{snydermastermind}
\nocite{neuwirth1982some}
\nocite{kooi2005yet}

\bibliographystyle{plain} 
\bibliography{project}

\end{document}
